<!doctype html>
<html lang="en">
<head>
  <title>SciOps Elements from ReproNim</title>
  <meta name="description" content="Slides for the ReproNim Webinar
									talk which walks through various
									automation aspects of a Reproducible Neuroimaging study workflow">
  <meta name="author" content=" Yaroslav O. Halchenko ">

  <meta charset="utf-8">
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="css/main.css" id="theme">
  <script src="js/printpdf.js"></script>
</head>
<body>

<div class="reveal">
<div class="slides">


<section>
<section>
  <a href="http://centerforopenneuroscience.org/"><img data-src="pics/con-ccn-dartmouth-letterhead.svg"></a>
  <h3>SciOps Elements from ReproNim</h3>

    <div style="margin-top:1em;text-align:center">
    <table style="border: none;">
    <tr>
	<td>
      Yaroslav O. Halchenko<br>
	  <small><a href="https://twitter.com/yarikoptic" target="_blank"><img data-src="pics/twitter.png" style="height:30px;margin:0px" />@yarikoptic</a></small>
	  <small><a href="https://fosstodon.org/@yarikoptic" target="_blank"><img data-src="pics/mastodon-icon.svg" style="height:30px;margin:0px;margin-right:0.1em" />@yarikoptic@fosstodon.org</a></small>
    </td>
	<td></td>
    </tr>
    <tr>
      <td>
		<small><br><a href="http://centerforopenneuroscience.org/" target="_blank">Center for Open Neuroscience</a>
          <br><a href="https://pbs.dartmouth.edu/" target="_blank">Department of Psychological and Brain Sciences</a>
          <br><a href="https://www.dartmouth.edu/ccn/" target="_blank">Center for Cognitive Neuroscience</a><br>
		  <a href="http://www.dartmouth.edu" target="_blank">Dartmouth College<br>
		  New Hampshire, USA</a></small>

		  <img style="width:200px; margin: 0px" data-src="pics/webinar-2024-reproflow-qrcode.png"/>
      </td>
	  <td>
	  </td>
    </tr>
    </table>
    </div>
<!--
  <p style="z-index: 100;position: fixed;background-color:#ede6d5;font-size:35px;box-shadow: 10px 10px 8px #888888;margin-top:0px;margin-bottom:100px;margin-left:1000px">
        <img src="pics/QRcode_hhu.png" height="200">
    </p>
<br><br><small>
    Slides: <a href="https://doi.org/10.5281/zenodo.6346849" target="_blank">
    DOI 10.5281/zenodo.6346849</a> (Scan the QR code)
    <br>
</small>
-->
<small>
  Live slides/<a href="https://github.com/ReproNim/webinar-2024-reproflow">Sources</a>: <a href="http://datasets.datalad.org/repronim/artwork/talks/webinar-2024-reproflow/#/">http://datasets.datalad.org/repronim/artwork/talks/webinar-2024-reproflow/</a>
<br>
 <!--	 <a href="http://pymvpa.org" target="_blank"> <img  style="height:140px;margin:2px" data-src="pics/pymvpa_icon.png"/></a> -->
	 <!-- <a href="http://repronim.org" target="_blank"> <img  style="height:140px;margin:2px"  data-src="pics/repronim-logo-vertical.svg"/></a> -->
	 <a href="http://neuro.debian.net" target="_blank"> <img  style="height:140px;margin:2px" data-src="pics/neurodebian.png"/></a>
	 <a href="https://open-brain-consent.readthedocs.io" target="_blank"> <img  style="height:140px;margin:2px"  data-src="pics/OBC_LogoCheck.svg"/></a>
     <a href="http://datalad.org" target="_blank"> <img  style="height:140px;margin:2px" data-src="pics/datalad_logo_posters_banner.svg"/></a>
	 <a href="https://github.com/myyoda/myyoda" target="_blank"> <img  style="height:140px;margin:2px" data-src="pics/yoda.svg"/></a>
	 <a href="https://dandiarchive.org" target="_blank"> <img  style="height:140px;margin:2px" data-src="pics/dandi-logo-square.svg"/></a>
  	 <a href="https://bids.neuroimaging.io/" target="_blank"> <img  style="height:140px;margin:2px" data-src="pics/BIDS_Logo.png"/></a>
	 <a href="https://standforukraine.com/" target="_blank"> <img  style="height:140px;margin:2px"  data-src="pics/Ukrainian_Blue-Yellow_ribbon.svg"/></a>

</small>
</section>


<section>
  <h2>Acknowledgments</h2>

	 <a href="https://github.com/myyoda/poster/blob/master/ohbm2018.pdf" target="_blank"> <img  style="height:150px;margin:20px" data-src="pics/yoda.svg"/></a>
	<img style="height:150px;margin-right:50px" data-src="pics/nsf1.jpg" />
	<img style="height:150px;margin-right:50px" data-src="pics/nih.png" />
<!--	<section    data-background-iframe="https://centerforopenneuroscience.org/whoweare#john_wodder_" data-background-interactive> -->
	 <iframe src="https://centerforopenneuroscience.org/whoweare#john_wodder_" style="width:100%; height:600px;"></iframe>
</section>
</section>
<section data-markdown  data-separator="^\n----\n" data-vertical="^\n---\n"><textarea data-template>

<!-- .element: class="fragment" data-fragment-index="1" -->
<img style="height:650px;margin-bottom:30px" data-src="pics/ReproSpectrum.png"/>

<small>From <a href="https://repronim.wordpress.com/2017/09/20/whatrepro/">"What is Reproducibility"</a>  by DN Kennedy.</small>


----

# Ultimate Goal/Approach

Reproducibility should become merely a *feature* <br/>(if not a
side-effect) of the *results*<br/><br/>
AKA "Reproducible by Design"
</textarea></section>

<section>
<section data-markdown  data-separator="^\n----\n" data-vertical="^\n---\n"><textarea data-template>


# But HOW?

----

## Introducing SciOps: Aim for Level 4

<!-- .slide: data-transition="fade" -->
![CMM Fig 1](pics/borrowed/CMM-Fig1.png)

<small>From <a href="https://arxiv.org/abs/2401.00077v1">"A Maturity Model for Operations in Neuroscience Research"</a>, 2023 by Erik C. Johnson, ..., Dimitri Yatsenko  <a href="https://arxiv.org/abs/2401.00077v1">https://arxiv.org/abs/2401.00077v1</a></small>

----

## Level 4: Scalable — Introducing SciOps

<!-- .slide: data-transition="fade" -->

<div id="birch" style="position: absolute;
                        top: -200px;
                        right: -200px;
                        width: 500px;
                        height: 400px;">
						<img src="pics/borrowed/CMM-Fig1.png"></img>
 </div>


Levels 4 and 5 transform research operations by adopting technologies
for **automation, scalability, and efficient collaboration**. We
collectively refer to these methodologies as SciOps, aligning with the
principles of other successful “Ops” disciplines in the technology
industry: DevOps, DataOps, and MLOps.

<small>From <a href="https://arxiv.org/abs/2401.00077v1">"A Maturity Model for Operations in Neuroscience Research"</a>, 2023 by Erik C. Johnson, ..., Dimitri Yatsenko  <a href="https://arxiv.org/abs/2401.00077v1">https://arxiv.org/abs/2401.00077v1</a></small>


----

# Where do we start?

<!-- .slide: data-transition="fade" -->

----

## HOWTO: Guiding principles
<!-- .slide: data-transition="fade" -->

- Be greedy
  - get as much as possible (even if you think you don't need it ATM)
- Be lazy
  - manually do as little as necessary
- Be not special
  - do not come up with *your ways*

----

## HOWTO: Guiding principles

<!-- .slide: data-transition="fade" -->

- Be ~~greedy~~ thorough
  - get as much as possible (even if you think you don't need it)
  - know what you are going to do and what you have done:
    - **automate provenance information collection**
- Be ~~lazy~~ efficient
  - manually do as little as necessary
    - **automate as much as feasible**
  - achieve more than originally planned
- Be ~~not special~~ ~~academic~~ formal
  - do not come up with *your ways*
    - **use standardized approaches**
  - become efficient collaborator

----

## One more: Pareto Principle


![Pareto principle](pics/webshot-pareto-search.jpg)

more: https://en.wikipedia.org/wiki/Pareto_principle

</textarea>
</section></section>
<section><section data-markdown  data-separator="^\n----\n" data-vertical="^\n---\n"><textarea data-template>

# What about a Recipe?

I was told that my Lasagna would be as great if I just follow the recipe... liers
 <!-- .element: style="font-size:0.6em" -->

----
## Recipe of a Study

- **Ingredients**:
  - humans  (100% of *effort*)
  - computers (0% of *effort*)
  - language(s) (100% of the *result*)
    - English/... (human -to- human)
    - programming/scripting languages (human -to- computer)
    - standards (human/computer -to- computer/human)

<!--

- **Steps**:
  - **humans**: plan the study ahead
  - **humans and computers**: do data collection/processing
-->

note:
- automate: use language(s) to make computers (not humans) do boring stuff
- human-to-human languages aren't good for automation

----

## ❮Languages-Rant❯

Part of the prior webinar "Reproducible Execution of Data Collection/Processing"

<small>

- Video: https://youtu.be/dwBtrpI2iS0
- Slides: https://datasets.datalad.org/repronim/artwork/talks/webinar-2020-reprocomp/#/4/2

</small>

### Main take away: Embrace standards

## ❮/Languages-Rant❯

----
## Recipe of a Study

- **Ingredients**:
  - humans  (100% of *effort*)
  - computers (0% of *effort*)
  - language(s) (100% of the *result*)
    - English/... (human -to- human)
    - programming/scripting languages (human -to- computer)
    - standards (human/computer -to- computer/human)
-   <!-- .element: class="fragment" data-fragment-index="2" -->
  **Steps**:
  - **humans**: plan the study ahead (**Science!**)
  - **humans and computers**: do data collection/processing (**SciOps!**)

note:
- automate: use language(s) to make computers (not humans) do boring stuff
- human-to-human languages aren't good for automation

----
### Currently Dominant Recipe Effort Proportions
<!-- .slide: data-transition="fade" -->

<div id="pareto" style="position: absolute;
                        top: -300px;
                        right: -200px;
                        width: 400px;
                        height: 400px;">
						<img src="pics/Paretos-80-20-rule.png"></img>
 </div>


- **Steps**:
  - **humans**: plan the study ahead (<20% effort)
  - **(a good number of) humans and (some) computers**:<br/> do data collection/processing (>80% effort)

----
### Target Recipe Effort Proportions
<!-- .slide: data-transition="fade" -->

**Prove Pareto to be *wrong*** <!-- .element: style="color:#ff0000" -->
<br/>**and that we can avoid wasting our effort** <!-- .element: style="color:#ff0000" -->

- **Steps**:
  - **humans**: plan the study ahead (>80% effort)
  - **(some) humans and (many) computers**: <br>
    automated data collection/processing (<20% effort)

![CMM Fig 1](pics/borrowed/CMM-Fig1.png)
<!-- .element: class="fragment" data-fragment-index="2" -->

----

### Recipe Steps&Ingredients for Planing Ahead (>80% effort)

![5 steps](pics/repronim-5steps.png)

----
### Humans: Plan Ahead (>80% effort)

- Plan to be ~~greedy~~ thorough
  - plan for **all** [5 ReproNim steps](http://5steps.repronim.org)
    (including *do-ing* analyses etc.)
    - prepare to be (ab)used ([Halchenko&Hanke, 2015](http://dx.doi.org/10.1186/s13742-015-0072-7))
    - be ~~lazy~~ **efficient** and learn from/(re)use work of others
	- improve your study workflow SciOps
      - check webinar from <a href="https://youtu.be/zFr8DyTGhxQ">Dec 2, 2022 by Keith Bush</a>
  - **choose an RDM (Research Data Management) platform/approach**
      - decide how to *log* what you will have done
  - **aim to collect rich(er) datasets**
- Pre-register
  - treat it as a checklist (now) and a "regression-test" (later)
- Prepare/train humans to "talk" to computers
  - [ReproNim Training](https://www.repronim.org/teach.html)
  - Listen to and/or participate in
    [BrainHacks](https://brainhack.org/tutorials.html)
  - [DataLad Handbook](http://handbook.datalad.org)

notes:
  - nothing is "final" until ... virtually never

</textarea></section></section>
<section><section data-markdown  data-separator="^\n----\n" data-vertical="^\n---\n"><textarea data-template>

# Plan Ahead: Choose an RDM

----

![RDM](pics/google-rdm.png)

----
### Plan Ahead: YODA

![YODA principles](pics/yoda-principles.png)

https://github.com/myyoda/poster/ <br/>by Michael Hanke (CP7, DataLad) et al.

----
### Plan Ahead: YODA's Layout

![YODA layout](pics/yoda-layout.png)

----

### Plan Ahead: YODA's Hierarchy

![YODA Hierarchy](pics/yoda-hierarchy-with-containers.png)

https://github.com/ReproNim/containers/

<!--
----

### Example: YODA's DataLad Reproducible Paper

![DataLad repropaper](pics/datalad-handbook-repropaper.png)

http://handbook.datalad.org/en/latest/usecases/reproducible-paper.html <br/>by
ReproNim YODA master Adina Wagner, Michael Hanke, et al.
-->

----

### Fact: ~~No~~Everybody should care about YODA

![fMRIPrep YODA](pics/fmriprep-yoda-PR.png)

https://fmriprep.org by Russ Poldrack (CP5*, OpenNeuro), et al.

----

### Plan Ahead: More on YODA via DataLad

![DataLad Handbook YODA](pics/datalad-handbook-yoda.png)

http://handbook.datalad.org/en/latest/basics/basics-yoda.html

----

### Plan Ahead: More on DataLad and Its Automations

[![Distribits 2024: What is in the DataLad Sandwich](pics/distribits-datalad-sandwich-whyvc.png)](https://www.youtube.com/watch?v=Mkb7qpYaL7o)

<small>

[distribits 2024](https://distribits.live). Yaroslav O. Halchenko. "What's in the DataLad sandwich" AKA DataLad "ecosystem":

- Video: https://www.youtube.com/watch?v=Mkb7qpYaL7o
- Slides: https://datasets.datalad.org/centerforopenneuroscience/talks/2024-distribits-datalad.html

</small>

----
### Plan Ahead: DataLad or not but ...

- become friends with YODA and its principles
- choose an RDM platform/approach which
  - tells you what you have done to obtain result X
  - knows where you have or can get data Y (of exact version Z)
  - **minimizes %effort** to re-run desired steps as-is or modified
  - works well with your *analytics* platforms
  - extra: allows you to search for data, results, etc
    - to find other *nominally similar*

</textarea></section></section>
<section><section data-markdown  data-separator="^\n----\n" data-vertical="^\n---\n"><textarea data-template>

# Plan Ahead:<br/> aim to collect rich(er) datasets

----

## Plan Ahead: automate collection of any relevant (meta)data in standard form

Re *automate*:
- manual data entry/wrangling = hard to trace/fix data bugs
- we must be efficient:
  - facilitate human/computer -to- computer/human interactions
  - we are not unique: (ab)use existing solutions
  - seek for longer term low %effort solutions

Re *any relevant*:
- of cause there is a trade-off
- prepare to be (ab)used
  - ~~others~~ you could find (meta)data relevant to their study missing
- more of explained variance = higher power
  - new explanations of "noise" regularly emerge
- have data ≠ have to analyze all data

Re *standard form*:
- without it - **high %effort** for a human/computer to understand it

</textarea></section></section>
<section><section data-markdown  data-separator="^\n----\n" data-vertical="^\n---\n"><textarea data-template>

## Plan Ahead: Collect DICOMs (not NIfTI, PAR/REC, ...)

- DICOMs contain lots of relevant metadata
  - most of the contained metadata is not relevant to your study
- Conversion from DICOMs to BIDS could be automated
- You should be able to
  - extract additional metadata happen you need it
  - reconvert if the conversion tool was buggy

----

### Plan Ahead: HeuDiConv/ReproIn

- **HeuDiConv** (https://github.com/nipy/heudiconv)

  - A flexible scriptable (Python) framework for conversion from
    DICOMs into an arbitrary layout
  - Uses [dcm2niix](https://github.com/rordenlab/dcm2niix/) by
    ReproNim Guru Chris Rorden for basic DICOM -> NIfTIs conversion
  - BIDS-aware and comes with a collection of conversion heuristics

----

### Plan Ahead: HeuDiConv/ReproIn

- **ReproIn** (https://github.com/repronim/reproin)
  - A convention for organizing and naming sequences on the scanner
    console
	- BIDS-like
	- **very low %effort to "adopt"**
  - HeuDiConv heuristic to convert from such convention to BIDS
  - Can be used to retroactively "map" into ReproIn (see <a
    href="https://github.com/ReproNim/reproin/issues/18">reproin/issues/18</a>)
  - Prototype <a
    href="https://github.com/ReproNim/reproin/blob/master/bin/reproin">reproin</a>
    script provides higher level functionality: study-create,
    study-convert, study-show, ...

----

### CRON Job: reproin lists-update-study-shows

![CRON Job email](pics/reproin-lists-update-study-show-email-20240607.png)

<small>

More about our setup at
https://github.com/ReproNim/reproin/tree/master?tab=readme-ov-file#complete-setup-at-dbic

</small>

----
### Plan Ahead: use ReproIn Convention

![DBIC conversions](pics/dbic-conversions.png)

----
### ReproIn Convention: Use ReproIn-namer by Will Foran

 <iframe src="https://npacore.github.io/reproin-namer/" style="width:100%; height:600px;"></iframe>


<small>

Use/contribute back: https://github.com/NPACore/reproin-namer
</small>

----
### Plan Ahead: ReproIn or not but automate conversion to BIDS!

- *Taking Control of your DICOM Data: ReproIn/Heudiconv Tools*
  webinar https://www.repronim.org/webinar-series.html
- Consider using reproin  helper to streamline handling of multiple studies
- If not ReproIn, consider
  <!--  - https://github.com/psychoinformatics-de/datalad-hirni (Hanke, CP7) -->
  - https://github.com/brainlife/ezbids (Pestilli, Brainlife, CP6*)
  - https://bids.neuroimaging.io/benefits.html#converters ...

</textarea></section></section>
<section><section data-markdown  data-separator="^\n----\n" data-vertical="^\n---\n"><textarea data-template>

## Plan Ahead: Get your ~~greedy~~ thorough hands on Phantom QA Data
### Surprise: Phantom QA data can explain (some)<br/> variance in your data!<br/>
(Operation code-name: ReproPhantom (?))

----

### FYI: Study "Nuisance"

![f1000](pics/f1000-webshot-20200930.png)

Cheng CP and Halchenko YO. A new virtue of phantom MRI data:
explaining variance in human participant data (v1; under peer
review). F1000Research 2020, 9:1131 <!-- .element: style="font-size:0.6em" -->
https://doi.org/10.12688/f1000research.24544.1

Full slide stack: <!-- .element: style="font-size:0.6em" -->
http://datasets.datalad.org/centerforopenneuroscience/nuisance/presentations/2020-NNL/

GitHub:  <!-- .element: style="font-size:0.6em" --> https://github.com/proj-nuisance/nuisance

<!--
----

### Planned Ahead: (somewhat)

- Phantom Data: DBIC QA ([///dbic/QA](http://datasets.datalad.org/?dir=/dbic/QA))
- Human Data: 206 participants from studies of 3 PIs at DBIC
- DICOM-to-BIDS: [HeuDiConv](https://github.com/nipy/heudiconv/) with [ReproIn](https://github.com/repronim/reproin/) heuristic
- Base OS: [Debian GNU/Linux](http://debian.org) + [NeuroDebian](http://neuro.debian.net)
- QA: [MRIQC](https://github.com/poldracklab/mriqc) (BIDS-App)
- Morphometrics: ReproNim's ["Simple Workflow"](https://github.com/ReproNim/simple_workflow)
  - [FSL](https://fsl.fmrib.ox.ac.uk/fsl/fslwiki): BET, FAST, FIRST
  - code/data/details: [10.12688/f1000research.10783.2](http://dx.doi.org/10.12688/f1000research.10783.2)
- Data wrangling and analyses:
  - [Python](http://python.org/), [pandas](https://pandas.pydata.org/), [statsmodels](https://www.statsmodels.org/stable/index.html), [Jupyter notebooks](https://jupyter.org/)
- Containerization: [Singularity](https://singularity.lbl.gov)
- Version control/distribution:
  - [DataLad](http://datalad.org), [datalad-container](http://handbook.datalad.org/en/latest/basics/101-133-containersrun.html?), [///ReproNim/containers](https://github.com/ReproNim/containers)
- Organization: follows [YODA principles](https://github.com/myyoda/poster/blob/master/ohbm2018.pdf)
  - https://github.com/proj-nuisance/nuisance
-->

----
### What we know: Phantoms are good for scanner QA

![SNR](./pics/snr_09142020.png)
<!-- .element: style="height: 500pt" -->

https://www.dartmouth.edu/dbic/research_infrastructure/qualityassurance.html
<!-- .element: style="font-size: 16pt" -->

and the variance is largely "noise", right?

----
### Model: Phantom SNR "explained"

![Fig 2](pics/f1000-webshot-20200930-fig2.png)

----
### Model: Phantom SNR (variables)

![Table 2](pics/f1000-webshot-20200930-tab2.png)

----
### But why do we care about "noise?"

![Power](pics/borrowed/somesite-power-analysis.png)


----
### Plan Ahead: Talk to your MR physicist/technician

- they better be doing QA
- do not discard phantom data - can come handy
  - improve you power
  - possibly help to harmonize across sites
- **ultra low %effort** to keep phantom data around
  - **∞ %effort** to recover when deleted
- **small %effort** to make use of it
  - concern: dates are "sensitive data"

</textarea></section></section>
<section><section data-markdown  data-separator="^\n----\n" data-vertical="^\n---\n"><textarea data-template>

## Plan Ahead: Physiological data

### Let's streamline acquisition of physiological data<br/>
(Operation code-name: ReproPhys)

----
### Others Planned it already: [phys2bids](https://github.com/physiopy/phys2bids/)

![phys2bids](pics/phys2bids-rtd.png)

A nice overview: [OHBM 2020 poster](https://cdn-akamai.6connex.com/645/1827//phys2bids_OHBM_15922384856589877.pdf)

----
### Others Planned it already: [bidsphysio](https://github.com/cbinyu/bidsphysio)

![bidsphysio](pics/bidsphysio-github.png)

HeuDiConv support PR: https://github.com/nipy/heudiconv/pull/446

----
### Plan Ahead: Consider collecting physiological data

- benefits are known
  - improve power of your studies
- tools for conversion/slicing are available
- relatively high **%effort** at the moment to setup/do
  - pales in comparison to other **%effort**s spent

</textarea></section></section>
<section><section data-markdown  data-separator="^\n----\n" data-vertical="^\n---\n"><textarea data-template>

## Plan Ahead: Automate collection of all related metadata (audio/video stimuli, events)

(Operation code-name: ReproFlow)

----

### ReproFlow: MRI Experiment *Flow

<!-- .slide: data-transition="fade" -->

![reproflow](pics/reproflow-blank.png)

----

### ReproFlow: Used Standards/Technologies

<!-- .slide: data-transition="fade" -->

![reproflow](pics/reproflow-techs1.png)

----

### ReproFlow: Projects

<!-- .slide: data-transition="fade" -->

![reproflow](pics/reproflow-projects1.png)

----
### ReproFlow: Projects

<!-- .slide: data-transition="fade" -->

<div id="birch" style="position: absolute;
                        top: -200px;
                        right: -200px;
                        width: 400px;
                        height: 400px;">
						<img src="pics/reproflow-projects1.png"></img>
 </div>

- **ReproStim** - capture audio and video stimuli
- **ReproEvents** - capture behavioral events from participants
- **Con/noisseur** - capture and QA operator input at the scanner console
- **ReproMon** - online operator feedback and alerts in case of incidents or anomalous metadata input


----

### Let's collect all video and audio stimuli as presented to the participants<br/>
(Operation code-name: ReproStim)

![](pics/reproflow-projects1-reprostim.png)

![](pics/borrowed/magewell-schema-1.jpg)

![](pics/borrowed/magewell-photo-1.jpg)

----
### But WHY/What For?

- QA (was there a jitter/dropped stimuli/randomization...)?
- make it possible to forward model **any** collected dataset
  - *resting state folks - see previous sections*
- explain low level signal features (/confounds?)
- post-hoc salience features analysis
- **100% reproduce experiment stimulation at ≈0% effort**

----
### Plowing Ahead: https://github.com/ReproNim/reprostim

- Goal: **0% effort** for "clients"
  - Minimal **%effort** to setup
  - Fully seamless and automated after that
- HOW:
  - [Video](https://www.amazon.com/gp/product/B00BLZDY6A/ref=ppx_yo_dt_b_search_asin_title?ie=UTF8&psc=1)/Audio splitters
  - Video/Audio grabber, e.g. [Magewell USB Capture DVI Plus](https://www.amazon.com/gp/product/B01MSDFAO5/)
  - A new video file upon connect/change of resolution<br>
	 (e.g., `2020.11.24.12.57.08_2020.11.24.15.51.23.mkv`   <!-- .element: style="font-size:0.8em" -->)
  - Synchronization:
    - NTP where possible <br/>(stimuli delivery computer, video grabber, ...)
	- video stream QR time-stamping and detection/decoding
  - Automated data synchronization/transfer using <a href="">git-annex assistant</a>
  - Target: Automated "slicing" of audio-video into BIDS datasets

</textarea></section></section>
<section><section data-markdown  data-separator="^\n----\n" data-vertical="^\n---\n"><textarea data-template>

### ReproFlow: Projects

<!-- .slide: data-transition="fade" -->

![reproflow](pics/reproflow-projects1.png)

----

## Plan Ahead: Events description

### Automate collection of all events information in consistent machine-readable form<br/>
(Operation code-name: ReproEvents)

![](pics/reproflow-projects1-reprostim.png)

----
### ReproEvents: Two approaches for Current Design hardware

![](pics/borrowed/curdes-932-1.png)    <!-- .element: style="width:40%" -->
![](pics/borrowed/curdes-birch-1.png) <!-- .element: style="width:40%" -->

- MicroPython board+code: collect events on 932 or Birch via AUX Port
- Native Birch device "tune up"

<small>

**Disclaimer**: we have no affiliation or commercial interest in the
curdes business

</small>


----
### ReproEvents: Birch Response collector

<div id="birch" style="position: absolute;
                        top: -200px;
                        right: -200px;
                        width: 400px;
                        height: 400px;">
						<img src="pics/borrowed/curdes-birch-1.png"></img>
 </div>

- https://www.curdes.com/birch-optical-interface-unit.html
- Contains Raspberry PI running [RaspberryPi](https://wiki.debian.org/RaspberryPi)
  - we installed NTP client to sync the clock
- Uses Python to run the GUI and record responses
  - we patched to record events as JSON records:

![birch events](pics/birch-events-1.png) <!-- .element: style="width:130%" -->

----

### ReproFlow: Projects

<!-- .slide: data-transition="fade" -->

![reproflow](pics/reproflow-projects1.png)

----

### Let's inform Experimenter about ongoing Repro-activities<br/>
(Operation code-name: ReproMon)

<!-- .slide: data-transition="fade" -->

![](pics/reproflow-projects1-repromon.png)

----
### ReproMon: Monitor the humans and hardware

<!-- .slide: data-transition="fade" -->

Information/alert feedback for Researcher/MRI tech.

![ReproMon](pics/repromon-20240607-1.png)

----

### ReproFlow: Take home summary

<div id="birch" style="position: absolute;
                        top: -200px;
                        right: -200px;
                        width: 400px;
                        height: 400px;">
						<img src="pics/reproflow-projects1.png"></img>
 </div>

- We are working on automation of collection of audio/video stimuli
  and responses/events timing and alignment them into BIDS datasets
- Sample collation of data are available from https://github.com/ReproNim/reproflow-data-sync
- Many components are ready for deployment by enthusiasts
- Coming: simplified installation on (Neuro)Debian-based systems
- Visit our exhibit table and poster on ReproFlow at OHBM 2024

</textarea></section></section>
<section><section data-markdown  data-separator="^\n----\n" data-vertical="^\n---\n"><textarea data-template>

## Plan Ahead (the 80%): Summary

- Plan to be ~~greedy~~ thorough
  - plan for **all** [5 ReproNim steps](http://5steps.repronim.org)
    (including *do-ing* analyses etc.)
    - be ~~lazy~~ efficient and (re)use work of others
  - **choose an RDM for your SciOps**
      - decide how to *log* what you will have done
  - **aim to automate collection of rich(er) datasets**
      - collect phantom data
      - consider adopting elements of ReproFlow
- Be ~~not special~~ ~~academic~~ formal
  - **learn about and reuse existing standards and study workflows**

</textarea></section></section>
<section><section data-markdown  data-separator="^\n----\n" data-vertical="^\n---\n"><textarea data-template>

# Reproducible Execution of Data Processing (the 20%)

Becomes more feasible with increased automation, <br/>
good RDM, and good (human- AND machine-) readable *logging*.

----

## The ultimate do it (the 20%)

- **(some) humans and (many) computers**: <br>
  automated data collection/processing (<20% effort)
  - data collection
  - QA
  - pre-processing
  - analysis
- publication composition <!-- .element: class="fragment" data-fragment-index="2" -->
  - also automate as much as possible by tuning prior stages
- keep in mind/plan for: <!-- .element: class="fragment" data-fragment-index="2" -->
  - pipelines are great, but could be tricky. Use pre-crafted:
    - MRIQC, fMRIPrep, C-PAC, ...
  - having a small number of modular steps allow for "ad-hoc" pipelining (minimal script
    etc)
  - containers can help to collaborate/scale/debug/...


----
### SciOP: ReproYODA with [ReproNim/containers](https://github.com/ReproNim/containers/#a-typical-workflow)

<!-- .slide: data-transition="fade" -->

![ReproNim/containers workflow](pics/yoda-hierarchy-with-containers.png)

----
### SciOP: ReproYODA with [ReproNim/containers](https://github.com/ReproNim/containers/#a-typical-workflow)

<!-- .slide: data-transition="fade" -->

![ReproNim/containers workflow](pics/repronim-containers-workflow.png)

----
### ReproYODA Log

![ReproNim/containers workflow show](pics/repronim-containers-show.png)

----
### Ultimately - many computers

![ReproMan run](pics/containers-reproman-only-mriqc.svg.png)

*Version control your data and computation using containers, DataLad and ReproMan, and reproducible they be!* <br/> https://www.repronim.org/webinar-series.html


----

### More on ReproMan

[![Distribits 2024: Joe on ReproMan](pics/distribits-reproman.png)](https://www.youtube.com/watch?v=P0SOGt3xWmw)

<small>

[distribits 2024](https://distribits.live). Joe Wexler: fMRI Pipelines on HPC with DataLad and ReproMan

- Video: https://www.youtube.com/watch?v=P0SOGt3xWmw

</small>

</textarea>


<!-- forgot that iframes to github do not work!
----

### Welcome the ultimate YODA style OpenNeuroDerivatives from CP5* OpenNeuro

	<iframe src="https://github.com/OpenNeuroDerivatives/"
    style="width:100%; height:600px;"></iframe>
-->

</section>

<section data-transition="None">
<h3>
YODA style OpenNeuro Derivatives from CP5* OpenNeuro
</h3>
	<small>
<center><p>Explore/obtain yourself from <a href="https://github.com/OpenNeuroDerivatives">https://github.com/OpenNeuroDerivatives</a></p></center>
</small>

	<div style="position: relative; width: 100%; height: 80vh;">

    <img src="pics/webshot-onderivs-listing.png" class="" width="80%" style="position: absolute; top: 0%; left: 0%" />
    <img src="pics/webshot-onderivs-gitmodules.png" class="fragment fade-in" width="80%"  style="position: absolute; top: 5%; left: 5%"  />
    <img src="pics/webshot-onderivs-repromanfolder.png" class="fragment fade-in" width="80%"  style="position: absolute; top: 10%; left: 10%"  />
    <img src="pics/webshot-onderivs-commandarray.png" class="fragment fade-in" width="80%"  style="position: absolute; top: 15%; left: 15%"  />
    <img src="pics/webshot-onderivs-stdout.png" class="fragment fade-in" width="80%"  style="position: absolute; top: 20%; left: 20%"  />

</div>
  </section>

</section>

<section data-markdown  data-separator="^\n----\n" data-vertical="^\n---\n"><textarea data-template>

## Welcome https://github.com/con/duct

![](pics/webshot-con-duct.png)

----

## duct as a runner within datalad containers-run

<div style="position: relative; width: 100%; height: 90vh;">

<img src="pics/screenshot-duct-diff.png" class="" width="80%" style="position: absolute; top: 0%; left: 0%" />
<img src="pics/screenshot-duct-1.png" class="fragment fade-in" width="100%" style="position: absolute; top: 0%; left: 0%" />
<img src="pics/screenshot-duct-2.png" class="fragment fade-in" width="100%" style="position: absolute; top: 25%; left: 0%" />
<img src="pics/screenshot-duct-3.png" class="fragment fade-in" width="100%" style="position: absolute; top: 15%; left: 10%" />
<img src="pics/screenshot-duct-4.png" class="fragment fade-in" width="100%" style="position: absolute; top: 60%; left: 10%" />


----

## Try duct for your SciOps!


</div>

</textarea>
</section>

<section data-markdown  data-separator="^\n----\n" data-vertical="^\n---\n"><textarea data-template>

## Final Take Home Message

<!-- .slide: data-transition="fade" -->

### Embrace SciOps

- Be ~~greedy~~ thorough
  - get as much as possible (even if you think you don't need it)
  - know what you are going to do and what you have done:
    - **automate provenance information collection**
- Be ~~lazy~~ efficient
  - manually do as little as necessary
    - **automate as much as feasible**
  - achieve more than originally planned
- Be ~~not special~~ ~~academic~~ formal
  - do not come up with *your ways*
    - **use standardized approaches**
  - become efficient collaborator

----

# Thank you!

## Establish your SciOps and
### Let the reproducibility be with you

<a href="https://github.com/myyoda/poster/blob/master/ohbm2018.pdf" target="_blank"> <img  style="height:450px;margin:20px" data-src="pics/yoda.svg"/></a>

Slides: https://github.com/ReproNim/webinar-2024-reproflow
<!-- .element: style="font-size:0.6em" -->

</textarea></section>
</section>

</div> <!-- /.slides -->
</div> <!-- /.reveal -->

<script src="reveal.js/js/reveal.js"></script>

<script>
  // Full list of configuration options available at:
  // https://github.com/hakimel/reveal.js#configuration
  Reveal.initialize({
    // The "normal" size of the presentation, aspect ratio will be preserved
    // when the presentation is scaled to fit different resolutions. Can be
    // specified using percentage units.
    width: 1280,
    height: 960,

    // Factor of the display size that should remain empty around the content
    margin: 0.1,

    // Bounds for smallest/largest possible scale to apply to content
    minScale: 0.2,
    maxScale: 1.0,

    controls: true,
    progress: true,
    history: true,
    center: true,

    transition: 'slide', // none/fade/slide/convex/concave/zoom
    // not supported? transition-speed: 'fast',

    // Optional reveal.js plugins
    dependencies: [
      { src: 'reveal.js/plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
      { src: 'reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
      { src: 'reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
      { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
      { src: 'reveal.js/plugin/notes/notes.js', async: true }
    ],
	 markdown: {
		 smartypants: true
	 }
  });
</script>
</body>
</html>
