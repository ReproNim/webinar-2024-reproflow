<!doctype html>
<html lang="en">
<head>
  <title>Reproducible Execution of Data Collection/Processing </title>
  <meta name="description" content="Slides for the ReproNim Webinar
									talk which walks through various
									aspects of a study to make it more
									reproducible ">
  <meta name="author" content=" Yaroslav O. Halchenko ">

  <meta charset="utf-8">
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="css/main.css" id="theme">
  <script src="js/printpdf.js"></script>
</head>
<body>

<div class="reveal">
<div class="slides">


<section>
<section>

  <a href="http://centerforopenneuroscience.org/"><img data-src="pics/con-ccn-dartmouth-letterhead.svg"></a>

  <h3>Reproducible Execution of Data Collection/Processing</h3>

  <!--
  <table style="border:none">
    <tr>
      <td>
		<img style="height:150px;margin-bottom:30px" data-src="pics/datalad_logo_wide.svg">
	  </td>
        <td>
            <h3>Use of MRI QC metrics, scanning parameters 
and seasonal variation data to control for variance in MRI studies</h3>
        </td>
    </tr>
</table> -->

  <div style="margin-top:0em;text-align:center">
  <table style="border: none;">
  <tr>
	<td>Yaroslav O. Halchenko
	  <br><small>
		<a href="https://twitter.com/yarikoptic" target="_blank">
		  <img data-src="pics/twitter.png" style="height:30px;margin:0px" />
		  @yarikoptic</a></small></td>
  </tr>
  <tr>
    <td>
        <small><a href="http://centerforopenneuroscience.org/" target="_blank">Center for Open Neuroscience</a>
          <br><a href="https://pbs.dartmouth.edu/" target="_blank">Department of Psychological and Brain Sciences</a>
          <br><a href="https://www.dartmouth.edu/ccn/" target="_blank">Center for Cognitive Neuroscience</a><br>
		  <a href="http://www.dartmouth.edu" target="_blank">Dartmouth College</a></small>
		<!--<img style="height:150px;" data-src="pics/con-logo_blue_big.svg">-->
    </td>
  </tr>
  </table>
  </div>
  <br>
    <small>
	 <a href="http://repronim.org" target="_blank"> <img  style="height:150px;margin:20px"  data-src="pics/repronim-logo-vertical.svg"/></a>
	 <a href="http://datalad.org" target="_blank"> <img  style="height:150px;margin:20px" data-src="pics/datalad_D.svg"/></a>
	 <a href="http://neuro.debian.net" target="_blank"> <img  style="height:150px;margin:20px" data-src="pics/neurodebian.png"/></a>
	 <a href="https://bids.neuroimaging.io" target="_blank"> <img  style="height:150px;margin:20px" data-src="pics/BIDS_Logo.png"/></a>
	 <a href="https://github.com/myyoda/poster/blob/master/ohbm2018.pdf" target="_blank"> <img  style="height:150px;margin:20px" data-src="pics/yoda.svg"/></a>
</small>
</section>

<!--
<section>
  <h2>Acknowledgements</h2>
  <table>
  <tr style="vertical-align:middle">
    <td style="vertical-align:middle">
      <dl>
        <dt style="margin-top:20px">QA data & DBIC support</dt>
        <dd style="margin-left:5px!important">
          <ul style="margin-left:5px!important">
            <li>Chandana Kodiweera</li>
			<li>Terry Sackett</li>
          </ul>
        </dd>
        <dt style="margin-top:20px">MRI data</dt>
        <dd style="margin-left:5px!important">
          <ul style="margin-left:5px!important">
			<li>Ida Gobbini</li>
			<li>James Haxby</li>
            <li>Luke Chang</li>
			<li>Eshin Jolly</li>
          </ul>
        </dd>
      </dl>
    </td>
	<td style="vertical-align:middle">
	  <div style="margin-left:40px;margin-bottom:-20px;text-align:center"><strong>Funders</strong></div>
	  <div style="margin-left:40px;text-align: left">
		<br>
		<img style="height:150px;margin-right:50px" data-src="pics/nsf1.jpg" />
		<ul style="margin-left:5px!important">
		  <li>1429999</li>
		  <li>1912266</li>
		  <li>1835200<br></li>
		</ul>
		<br><br><img style="height:150px;margin-right:50px" data-src="pics/nih.png" />
          <ul style="margin-left:5px!important">
            <li>P41EB019936</li>
			<li>R01MH116026</li>
			<li>R56MH080716</li>
			<li>R01MH116026</li>
		  </ul>
		  </div>
	</td>
  </tr>
</div>
<!- -
  <div style="margin-top:40px;margin-bottom:20px;text-align:center"><strong>Collaborators</strong></div>
  <div style="margin-top:-20px">
  <img style="height:100px;margin:20px" data-src="pics/hbp_logo.png" />
  <img style="height:100px;margin:20px" data-src="pics/conp_logo.png" />
  <img style="height:100px;margin:20px" data-src="pics/vbc_logo.png" />
  </div>
  <div style="margin-top:-40px">
  <img style="height:120px;margin:20px" data-src="pics/openneuro_logo.png" />
  <img style="height:120px;margin:20px" data-src="pics/cbrain_logo.png" />
  <img style="height:140px;margin:20px" data-src="pics/brainlife_logo.png" />
  </div>
  - ->
  </td>
  </tr>
  </table>
</section>
-->
</section>

<section data-markdown  data-separator="^\n----\n" data-vertical="^\n---\n"><textarea data-template>

<!-- .element: class="fragment" data-fragment-index="1" -->
<img style="height:650px;margin-bottom:30px" data-src="pics/ReproSpectrum.png">


- Reproducible Execution ⊃ Re-Execution
  - need *Exact Same* or *Nominally Similar* Data
  - need *Exact Same* or *Nominally Similar* Analysis
- Reproducible Execution is useful to Original
  <!-- .element: class="fragment" data-fragment-index="3" -->
  - rarely research project is "linear"

<!-- .element: class="fragment" data-fragment-index="2" -->


----

# Ultimate Goal/Approach

Reproducibility should become merely a *feature* <br/>(if not a
side-effect) of the *results*<br/><br/>
AKA "Reproducible by Design"
</textarea></section>

<section>
<section data-markdown  data-separator="^\n----\n" data-vertical="^\n---\n"><textarea data-template>


# But HOW?

<!-- .slide: data-transition="fade" -->

----


## HOWTO: Guiding principles
<!-- .slide: data-transition="fade" -->

- Be greedy
  - get as much as possible (even if you think you don't need it ATM)
- Be lazy
  - manually do as little as necessary

----

## HOWTO: Guiding principles

<!-- .slide: data-transition="fade" -->

- Be ~~greedy~~ thorough
  - get as much as possible (even if you think you don't need it)
  - know what you are going to do and what you have done
- Be ~~lazy~~ efficient
  - manually do as little as necessary
  - achieve more than originally planned

----

## One more: Pareto Principle


![Pareto principle](pics/webshot-pareto-search.jpg)

more: https://en.wikipedia.org/wiki/Pareto_principle

</textarea>
</section></section>
<section><section data-markdown  data-separator="^\n----\n" data-vertical="^\n---\n"><textarea data-template>

# What about a Recipe?

I was told that my Lasagna would be as great if I just follow the recipe... liers.
 <!-- .element: style="font-size:0.6em" -->

----
## Recipe of a Study

<div id="pareto" style="position: absolute;
                        top: -20px;
                        right: 0px;
                        width: 400px;
                        height: 400px;">
						<img src="pics/Paretos-80-20-rule.png"></img>
 </div>

- **Ingredients**:
  - humans  (100% of *effort*)
  - computers (0% of *effort*)
  - language(s) (100% of the *result*)
    - English/... (human -to- human)
    - programming/scripting languages (human -to- computer)
    - standards (human/computer -to- computer/human)

<!--

- **Steps**:
  - **humans**: plan the study ahead
  - **humans and computers**: do data collection/processing
-->

note:
- automate: use language(s) to make computers (not humans) do boring stuff
- human-to-human languages aren't good for automation

----

# Languages

----

## Quick take aways

- Human languages ... suck (unless you are into Poetry or Literature)
  - https://www.cognitiveatlas.org by Russ Poldrack (CP5*)
  - https://neuinfo.org/interlex by TR&D1 group
  - ... but are the ones for README
- Methods & Results *should be* low %effort collation of automatically
  generated materials in both human- AND computer- readable forms
  - not unprecedented: FSL, fMRIPrep, ...
- The *best* Programming language is the one which minimizes %effort
  - Corollary: <br/>%effort is minimized by using no Programming language

----

## ?

----

## Standards

*The problem with Programming:*

- It takes **huge %effort** to write/test/maintain a program
- A program typically creates a new "language" for interaction with it
  - it takes **%effort** to learn a new program
  - a program cannot learn to *talk* to another program,<br/> so it takes
    **%effort** to script "adapter" or "pipeline" communication between<br/>
	(thank you NiPype and TR&D2 for helping out!)

----

## Standards

*To **minimize %effort** for human/computer -to- computer/human interactions*:

- reuse and contribute to existing programs
- make programs operate on and produce data in standard form(s):
 - BIDS (Derivatives, Models, ...), NIDM, HED, ...
 - DICOM, NIfTI, JSON, YAML, ...
- make programs interface via standard(ized) interface(s):
 - BIDS-Apps, ABC apps, Boutiques, Flywheel Gears, ...
- contribute to development of standards to
 - become part of their 80% coverage
 - **minimize the %effort to produce results in a standard form so<br/>
  they could be re-used with minimal %effort to produce new results**

----



# Main take away: <br/>Embrace standards

## </Languages Rant>

----
## Recipe of a Study

<div id="pareto" style="position: absolute;
                        top: -20px;
                        right: 0px;
                        width: 400px;
                        height: 400px;">
						<img src="pics/Paretos-80-20-rule.png"></img>
 </div>

- **Ingredients**:
  - humans  (100% of *effort*)
  - computers (0% of *effort*)
  - language(s) (100% of the *result*)
    - English/... (human -to- human)
    - programming/scripting languages (human -to- computer)
    - standards (human/computer -to- computer/human)
-   <!-- .element: class="fragment" data-fragment-index="2" -->
  **Steps**:
  - **humans**: plan the study ahead
  - **humans and computers**: do data collection/processing

note:
- automate: use language(s) to make computers (not humans) do boring stuff
- human-to-human languages aren't good for automation

----
### Currently Dominant Recipe Effort Proportions
<!-- .slide: data-transition="fade" -->

- **Steps**:
  - **humans**: plan the study ahead (<20% effort)
  - **(a good number of) humans and (some) computers**:<br/> do data collection/processing (>80% effort)

----
### Target Recipe Effort Proportions
<!-- .slide: data-transition="fade" -->

**Prove Pareto to be *wrong*** <!-- .element: style="color:#ff0000" -->
<br/>**and that we can avoid wasting our effort** <!-- .element: style="color:#ff0000" -->

- **Steps**:
  - **humans**: plan the study ahead (>80% effort)
  - **(some) humans and (many) computers**: <br>
    automated data collection/processing (<20% effort)

----

### Recipe Steps&Ingredients for Planing Ahead (>80% effort)

![5 steps](pics/repronim-5steps.png)

----
### Humans: Plan Ahead (>80% effort)

- Plan to be ~~greedy~~ thorough
  - plan for **all** [5 ReproNim steps](http://5steps.repronim.org)
    (including *do-ing* analyses etc.)
    - prepare to be (ab)used ([Halchenko&Hanke, 2015](http://dx.doi.org/10.1186/s13742-015-0072-7))
    - be ~~lazy~~ efficient and (re)use work of others
    - choose what to (ab)use and possibly contribute to:
      - language(s), analysis&execution platforms, ...
  - **choose an RDM (Research Data Management) platform/approach**
      - decide how to *log* what you will have done
  - **aim to collect rich(er) datasets**
- Pre-register
  - treat it as a checklist (now) and a "regression-test" (later)
- Prepare/train humans to "talk" to computers
  - [ReproNim Training](https://www.repronim.org/teach.html)
  - Listen to and/or participate in
    [BrainHacks](https://brainhack.org/tutorials.html)
  - [DataLad Handbook](http://handbook.datalad.org)

notes:
  - nothing is "final" until ... virtually never

</textarea></section></section>
<section><section data-markdown  data-separator="^\n----\n" data-vertical="^\n---\n"><textarea data-template>

# Plan Ahead: Choose an RDM

----

![RDM](pics/google-rdm.png)

----
### Plan Ahead: YODA

![YODA principles](pics/yoda-principles.png)

https://github.com/myyoda/poster/ <br/>by Michael Hanke (CP7, DataLad) et al.

----
### Plan Ahead: YODA's Layout

![YODA layout](pics/yoda-layout.png)

----

### Plan Ahead: YODA's Hierarchy

![YODA Hiearchy](pics/yoda-hierarchy-with-containers.png)

https://github.com/ReproNim/containers/

----

### Example: YODA's DataLad Reproducible Paper

![DataLad repropaper](pics/datalad-handbook-repropaper.png)

http://handbook.datalad.org/en/latest/usecases/reproducible-paper.html <br/>by
ReproNim YODA master Adina Wagner, Michael Hanke, et al.

----

### Fact: ~~No~~Everybody should care about YODA

![fMRIPrep YODA](pics/fmriprep-yoda-PR.png)

https://fmriprep.org by Russ Poldrack (CP5*, OpenNeuro), et al.

----

### Plan Ahead: More on YODA via DataLad

![DataLad Handbook YODA](pics/datalad-handbook-yoda.png)

http://handbook.datalad.org/en/latest/basics/basics-yoda.html

----
### Plan Ahead: DataLad or not but ...

- become friends with YODA and its principles
- choose an RDM platform/approach which
  - tells you what you have done to obtain result X
  - knows where you have or can get data Y (of exact version Z)
  - **minimizes %effort** to re-run desired steps as-is or modified
  - works well with your *analytics* platforms
  - extra: allows you to search for data, results, etc
    - to find other *nominally similar*

</textarea></section></section>
<section><section data-markdown  data-separator="^\n----\n" data-vertical="^\n---\n"><textarea data-template>

# Plan Ahead:<br/> aim to collect rich(er) datasets

----

## Plan Ahead: automate collection of any relevant (meta)data in standard form

Re *automate*:
- manual data entry/wrangling = hard to trace/fix data bugs
- we must be efficient:
  - facilitate human/computer -to- computer/human interactions
  - we are not unique: (ab)use existing solutions
  - seek for longer term low %effort solutions

Re *any relevant*:
- of cause there is a trade-off
- prepare to be (ab)used
  - ~~others~~ you could find (meta)data relevant to their study missing
- more of explained variance = higher power
  - new explanations of "noise" regularly emerge
- have data ≠ have to analyze all data

Re *standard form*:
- without it - **high %effort** for a human/computer to understand it

----

## Plan Ahead: Prior Webinars on "Languages" at tools to master them

- *Harmonizing clinical and behavioral data collection through ReproSchema* by Satra Ghosh
- *Tools and Techniques for BIDS Semantic Annotation and Query Across Datasets with NIDM* by David Keator
- *Damn it Jim, I am a researcher not an ontologist: Exploring and using terminologies* by Jeff Grethe
- ... and more at https://www.repronim.org/webinar-series.html


</textarea></section></section>
<section><section data-markdown  data-separator="^\n----\n" data-vertical="^\n---\n"><textarea data-template>

## Plan Ahead: Collect DICOMs (not NIfTI, PAR/REC, ...)

- DICOMs contain lots of relevant metadata
  - most of the contained metadata is not relevant to your study
- Conversion from DICOMs to BIDS could be automated
- You should be able to
  - extract additional metadata happen you need it
  - reconvert if the conversion tool was buggy

----

### Plan Ahead: HeuDiConv/ReproIn

- **HeuDiConv** (https://github.com/nipy/heudiconv)

  - A flexible scriptable (Python) framework for conversion from
    DICOMs into an arbitrary layout
  - Uses [dcm2niix](https://github.com/rordenlab/dcm2niix/) by
    ReproNim Guru Chris Rorden for basic DICOM -> NIfTIs conversion
  - BIDS-aware and comes with a collection of conversion heuristics

- **ReproIn** (https://github.com/repronim/reproin)
  - A convention for organizing and naming sequences on the scanner
    console
	- BIDS-like
	- **very low %effort to "adopt"**
  - HeuDiConv heuristic to convert from such convention to BIDS

----
### Plan Ahead: use ReproIn Convention

![DBIC conversions](pics/dbic-conversions.png)

----
### Plan Ahead: ReproIn or not but automate conversion to BIDS!

- *Taking Control of your DICOM Data: ReproIn/Heudiconv Tools*
  webinar https://www.repronim.org/webinar-series.html
- WiP: reproin  helper to streamline handling of multiple studies
- If not ReproIn, consider
  - https://github.com/psychoinformatics-de/datalad-hirni (Hanke, CP7)
  - https://github.com/brainlife/ezbids (Pestilli, Brainlife, CP6*)
  - https://bids.neuroimaging.io/benefits.html#converters ...

</textarea></section></section>
<section><section data-markdown  data-separator="^\n----\n" data-vertical="^\n---\n"><textarea data-template>

## Plan Ahead: Get your ~~greedy~~ thorough hands on Phantom QA Data
### Surprise: Phantom QA data can explain (some)<br/> variance in your data!<br/>
(Operation code-name: ReproPhantom (?))

----

### FYI: Study "Nuisance"

![f1000](pics/f1000-webshot-20200930.png)

Cheng CP and Halchenko YO. A new virtue of phantom MRI data:
explaining variance in human participant data (v1; under peer
review). F1000Research 2020, 9:1131 <!-- .element: style="font-size:0.6em" -->
https://doi.org/10.12688/f1000research.24544.1

Full slide stack: <!-- .element: style="font-size:0.6em" -->
http://datasets.datalad.org/centerforopenneuroscience/nuisance/presentations/2020-NNL/

GitHub:  <!-- .element: style="font-size:0.6em" --> https://github.com/proj-nuisance/nuisance

----

### Planned Ahead: (somewhat)

- Phantom Data: DBIC QA ([///dbic/QA](http://datasets.datalad.org/?dir=/dbic/QA))
- Human Data: 206 participants from studies of 3 PIs at DBIC
- DICOM-to-BIDS: [HeuDiConv](https://github.com/nipy/heudiconv/) with [ReproIn](https://github.com/repronim/reproin/) heuristic
- Base OS: [Debian GNU/Linux](http://debian.org) + [NeuroDebian](http://neuro.debian.net)
- QA: [MRIQC](https://github.com/poldracklab/mriqc) (BIDS-App)
- Morphometrics: ReproNim's ["Simple Workflow"](https://github.com/ReproNim/simple_workflow)
  - [FSL](https://fsl.fmrib.ox.ac.uk/fsl/fslwiki): BET, FAST, FIRST
  - code/data/details: [10.12688/f1000research.10783.2](http://dx.doi.org/10.12688/f1000research.10783.2)
- Data wrangling and analyses:
  - [Python](http://python.org/), [pandas](https://pandas.pydata.org/), [statsmodels](https://www.statsmodels.org/stable/index.html), [Jupyter notebooks](https://jupyter.org/)
- Containerization: [Singularity](https://singularity.lbl.gov)
- Version control/distribution:
  - [DataLad](http://datalad.org), [datalad-container](http://handbook.datalad.org/en/latest/basics/101-133-containersrun.html?), [///ReproNim/containers](https://github.com/ReproNim/containers)
- Organization: follows [YODA principles](https://github.com/myyoda/poster/blob/master/ohbm2018.pdf)
  - https://github.com/proj-nuisance/nuisance

----
### What we know: Phantoms are good for scanner QA

![SNR](./pics/snr_09142020.png)
<!-- .element: style="height: 500pt" -->

https://www.dartmouth.edu/dbic/research_infrastructure/qualityassurance.html
<!-- .element: style="font-size: 16pt" -->

and the variance is largely "noise", right?

----
### Model: Phantom SNR "explained"

![Fig 2](pics/f1000-webshot-20200930-fig2.png)

----

### Model: Phantom SNR (variables)

![Table 2](pics/f1000-webshot-20200930-tab2.png)

----

### Model: Gray brain matter

![Fig 3](pics/f1000-webshot-20200930-fig3.png)

----

### Model: Gray brain matter (variables)

![Table 3](pics/f1000-webshot-20200930-tab3.png)

----
### Plan Ahead: Talk to your MR physicist/technician

- they better be doing QA
- do not discard phantom data - can come handy
  - improve you power
  - possibly help to harmonize across sites
- **ultra low %effort** to keep phantom data around
  - **∞ %effort** to recover when deleted
- **small %effort** to make use of it
  - concern: dates are "sensitive data"

</textarea></section></section>
<section><section data-markdown  data-separator="^\n----\n" data-vertical="^\n---\n"><textarea data-template>

## Plan Ahead: Physiological data

### Let's streamline acquisition of physiological data<br/>
(Operation code-name: ReproPhys)

----
### Others Planned it already: [phys2bids](https://github.com/physiopy/phys2bids/)

![phys2bids](pics/phys2bids-rtd.png)

A nice overview: [OHBM 2020 poster](https://cdn-akamai.6connex.com/645/1827//phys2bids_OHBM_15922384856589877.pdf)

----
### Others Planned it already: [bidsphysio](https://github.com/cbinyu/bidsphysio)

![bidsphysio](pics/bidsphysio-github.png)

HeuDiConv support PR: https://github.com/nipy/heudiconv/pull/446

----
### Plan Ahead: Consider collecting physiological data

- benefits are known
  - improve power of your studies
- tools for conversion/slicing are available
- relatively high **%effort** at the moment to setup/do
  - pales in comparison to other **%effort**s spent

</textarea></section></section>
<section><section data-markdown  data-separator="^\n----\n" data-vertical="^\n---\n"><textarea data-template>

## Plan Ahead: "Raw" audio/video stimuli

### Let's collect all video and audio stimuli as presented to the participants<br/>
(Operation code-name: ReproStim)

----
### But WHY/What For?

- QA (was there a jitter/dropped stimuli/randomization...)?
- make it possible to forward model **any** collected dataset
  - *resting state folks - see previous sections*
- explain low level signal features (/confounds?)
- post-hoc salience features analysis
- **100% reproduce experiment stimulation at ≈0% effort**

----
### Plowing Ahead: https://github.com/ReproNim/reprostim

- Goal: **0% effort** for "clients"
  - Minimal **%effort** to setup
  - Fully seamless and automated after that
- HOW:
  - [Video](https://www.amazon.com/gp/product/B00BLZDY6A/ref=ppx_yo_dt_b_search_asin_title?ie=UTF8&psc=1)/Audio splitters
  - Video/Audio grabber, e.g. [Magewell USB Capture DVI Plus](https://www.amazon.com/gp/product/B01MSDFAO5/)
    - loseless video codec
  - A new video file upon connect/change of resolution<br>
	 (e.g., `2020.11.24.12.57.08_2020.11.24.15.51.23.mkv`   <!-- .element: style="font-size:0.8em" -->)
  - Synchronization:
    - NTP where possible <br/>(stimuli delivery computer, video grabber, ...)
	- video stream QR time-stamping and detection/decoding
  - Automated "slicing" into BIDS datasets (WiP)

</textarea></section></section>
<section><section data-markdown  data-separator="^\n----\n" data-vertical="^\n---\n"><textarea data-template>

## Plan Ahead: Events description

### Automate collection of all events information in consistent machine-readable form<br/>
(Operation code-name: ReproEvents)

----
### Still Planning Ahead: Join us

- **There is no generic library/helper yet**
- Target: helper+converters for PsychoPy/PTB-3 to produce rich<br/>
  BIDS [`_events.tsv` + `_events.json`](https://bids-specification.readthedocs.io/en/stable/04-modality-specific-files/07-behavioral-experiments.html)<br/>
  with [Hierarchical Event Descriptors (HED)](https://github.com/hed-standard/hed-specification)
- But with **little %effort** you should already make you stimuli
  scripts/logs ready<br/> (see e.g., https://github.com/mvdoc/pitcher_localizer)

</textarea></section></section>
<section><section data-markdown  data-separator="^\n----\n" data-vertical="^\n---\n"><textarea data-template>

## Plan Ahead (the 80%): Summary

TODO

</textarea></section></section>
<section><section data-markdown  data-separator="^\n----\n" data-vertical="^\n---\n"><textarea data-template>

# Reproducible Execution of Data Processing

Becomes more feasible with increased automation, <br/>
good RDM, and good (human- AND machine-) readable *logging*.

----

## The ultimate do it (the 20%)

- **(some) humans and (many) computers**: <br>
  automated data collection/processing (<20% effort)
  - data collection
  - QA
  - pre-processing
  - analysis

----

## Random Example: [srndna-master](https://github.com/victoriaakelly/srndna-master)

<div id="victoriaakelly" style="position: absolute;
                        top: -20px;
                        right: 0px;
                        width: 200px;
                        height: 200px;">
						<img src="https://avatars3.githubusercontent.com/u/35232563?s=400&u=db58024a7062472bc798abbdc7183672fb9ec84b&v=4"></img>
 </div>

![README](pics/example-workflow-readme.png)

----

## The ultimate do it (the 20%)

- **(some) humans and (many) computers**: <br>
  automated data collection/processing (<20% effort)
  - data collection
  - QA
  - pre-processing
  - analysis
- publication composition <!-- .element: class="fragment" data-fragment-index="2" -->
  - also automate as much as possible by tuning prior stages
- keep in mind/plan for: <!-- .element: class="fragment" data-fragment-index="2" -->
  - everything might need to be 'repeated'
  - pipelines are great, but could be tricky. Use pre-crafted:
    - MRIQC, fMRIPrep, C-PAC, ...
  - having a small number of modular steps allow for "ad-hoc" pipelining (minimal script
    etc)
  - containers can help to collaborate/scale/debug/...

----
### ReproYODA approach with [ReproNim/containers](https://github.com/ReproNim/containers/#a-typical-workflow)

![ReproNim/containers workflow](pics/repronim-containers-workflow.png)

----
### ReproYODA Log

![ReproNim/containers workflow show](pics/repronim-containers-show.png)

----
### Ultimately - many computers

![ReproMan run](pics/containers-reproman-only-mriqc.svg.png)

*Version control your data and computation using containers, DataLad and ReproMan, and reproducible they be!* <br/> https://www.repronim.org/webinar-series.html


</textarea>
</section>
</section>



<section>
 <section data-markdown  data-separator="^\n----\n" data-vertical="^\n---\n"><textarea data-template>

</textarea></section></section>



<section>
    <h2>Further Information</h2>
    <ul>
      <li>Data/code:
		<ul>
		  <li><a href="https://github.com/proj-nuisance">github.com/proj-nuisance</a> </li>
		  <li><a href="http://datasets.datalad.org/?dir=/con/nuisance">datasets.datalad.org/?dir=/con/nuisance</a></li>
		  </ul>
      <li>DataLad: <a href="https://datalad.org">datalad.org</a></li>
	  <li>ReproNim: <a href="https://repronim.org">repronim.org</a></li>
    <li>
	  Slides: <ul><li>"Sources": <a href="https://github.com/proj-nuisance/talk-2020-NNL" target="_blank">
			https://github.com/proj-nuisance/talk-2020-NNL</a></li>
		<li>View <a href="http://datasets.datalad.org/con/nuisance/presentations/2020-NNL/#/">here</a></li>
	  </ul>
	</ul>

</section>

 <section data-markdown  data-separator="^\n----\n" data-vertical="^\n---\n"><textarea data-template>
# Thank you<br>for hanging till the End!

## Let the reproducibility be with you

<a href="https://github.com/myyoda/poster/blob/master/ohbm2018.pdf" target="_blank"> <img  style="height:450px;margin:20px" data-src="pics/yoda.svg"/></a>

</textarea></section>

<section>

</section>
</section>

</div> <!-- /.slides -->
</div> <!-- /.reveal -->

<script src="reveal.js/js/reveal.js"></script>

<script>
  // Full list of configuration options available at:
  // https://github.com/hakimel/reveal.js#configuration
  Reveal.initialize({
    // The "normal" size of the presentation, aspect ratio will be preserved
    // when the presentation is scaled to fit different resolutions. Can be
    // specified using percentage units.
    width: 1280,
    height: 960,

    // Factor of the display size that should remain empty around the content
    margin: 0.1,

    // Bounds for smallest/largest possible scale to apply to content
    minScale: 0.2,
    maxScale: 1.0,

    controls: true,
    progress: true,
    history: true,
    center: true,

    transition: 'slide', // none/fade/slide/convex/concave/zoom
    // not supported? transition-speed: 'fast',

    // Optional reveal.js plugins
    dependencies: [
      { src: 'reveal.js/plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
      { src: 'reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
      { src: 'reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
      { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
      { src: 'reveal.js/plugin/notes/notes.js', async: true }
    ],
	 markdown: {
		 smartypants: true
	 }
  });
</script>
</body>
</html>
